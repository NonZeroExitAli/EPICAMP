1- Classification Module

Architecture

Data Sources: The datasets used include APD3, DRAMP, ADAM, CAMP, and dbAMP, which provide sequences of antimicrobial peptides (AMPs). For non-AMPs, sequences were extracted from available datasets and curated to serve as negative examples in the classification task.

Preprocessing: The preprocessing pipeline included multiple steps to ensure data quality. Any sequences containing unknown amino acids (e.g., U, O, B, Z, J) or whitespace were removed. CD-HIT was used to reduce sequence redundancy by clustering similar sequences with a similarity threshold of 90%. Finally, features were normalized using MinMax scaling for uniformity.

Feature Extraction: Four types of features were extracted to represent peptide sequences numerically:
  - Amino Acid Composition (AAC) quantified the frequency of amino acids.
  - Autocorrelation described the distribution of amino acids and their properties.
  - Composition, Transition, and Distribution (CTD) captured sequence properties.
  - Pseudo-Amino Acid Composition (PseAAC) encoded sequence-order effects and physicochemical properties.

Feature Combinations: Feature subsets were tested in combinations ranging from single methods to pairs and higher-order combinations. This ensured comprehensive exploration of feature interactions and their impact on model performance.

Feature Selection: Various feature selection methods were employed to reduce dimensionality and enhance model performance:
  - Recursive Feature Elimination (RFE), Variance Threshold (VT), Random Forest, and Boruta.
  - These methods were tested individually and in combinations to optimize feature subsets.

Machine learning models:  Support Vector Machine (SVM): A supervised learning algorithm used for classification tasks. Random Forest Classifier: An ensemble learning method that builds multiple decision trees and combines their outputs to improve classification accuracy. K-Nearest Neighbors (KNN): A classification algorithm that assigns class labels based on the majority vote of the k nearest neighbors.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Design Decisions

Data Cleaning: Sequences were filtered to ensure consistency for downstream analysis. Only sequences with lengths ranging between 10 and 100 amino acids were retained. This range was chosen based on the literature to exclude very short sequences that might lack meaningful structural information, as well as very long sequences that could introduce noise.

Feature Selection Thresholds: The threshold for feature importance in the Random Forest model was set to the mean value of the feature importances. For the Variance Threshold method, features with variances in the top 5% were retained. To minimize computational complexity, the maximum number of iterations for Boruta was set to 10. Recursive Feature Elimination (RFE) selected features based on cross-validation, optimizing for accuracy during the process.

Models’ Parameters: All parameters in this section were determined using grid search to optimize model performance:
  - The Support Vector Machine (SVM) was configured with an RBF kernel, using C=100 and gamma='scale'.
  - The Random Forest Classifier was set with a maximum depth of 40, max_features='sqrt', and a minimum of one sample per leaf.
  - For the K-Nearest Neighbors (KNN) algorithm, the Manhattan distance metric was employed, with the number of neighbors (k) set to 1.

Imbalanced Data: The Synthetic Minority Over-sampling Technique (SMOTE) was implemented to address class imbalance by generating synthetic data points for the minority class. This approach ensured that the AMP and non-AMP classes were balanced, enhancing the reliability of the machine learning models during training.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Algorithms Used

Feature Selection Algorithms

  - Recursive Feature Elimination (RFE): An iterative feature selection algorithm that fits a model, ranks features based on importance, and removes the least important features.
  - Variance Threshold (VT): A feature selection method that eliminates features with low variance.
  - Boruta: A wrapper-based feature selection algorithm that uses Random Forest to evaluate the importance of features and identifies all relevant ones.
  - andom Forest Feature Importance**: A feature selection approach that uses the feature importance scores generated by the Random Forest algorithm, ranking features based on their contribution to model             performance.

ML Models

  - Support Vector Machine (SVM): A supervised learning algorithm used for classification tasks.
  - Random Forest Classifier: An ensemble learning method that builds multiple decision trees and combines their outputs to improve classification accuracy.
  - K-Nearest Neighbors (KNN): A classification algorithm that assigns class labels based on the majority vote of the k nearest neighbors.

Balancing Algorithms

  - SMOTE: A data augmentation algorithm used to address class imbalance by generating synthetic data points for the minority class.

Explainability Algorithms

  - LIME: An explainability algorithm used to interpret the predictions of machine learning models. LIME explains individual predictions by approximating the model locally with an interpretable surrogate model.
  - SHAP: An explainability algorithm used to interpret the predictions globally of machine learning models.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Dependencies

  - Python Libraries: pandas, numpy, matplotlib, scikit-learn, imblearn, lime, propy, shap
  - External Tools: CD-HIT for sequence clustering
  - Environment: Google Colab with GPU for efficient computation



2- Regression Module

Architecture

Data Sources:  
The datasets used in this study include curated antimicrobial peptide (AMP) sequences and their experimentally measured MIC values for four organisms: E. coli, Staphylococcus aureus, Pseudomonas aeruginosa, and Pneumoniae. The datasets were collected from public repositories and preprocessed to ensure high-quality data for regression modeling.

Preprocessing:  
The preprocessing pipeline included multiple steps:  
- Removal of sequences containing unknown amino acids or non-standard characters.  
- Conversion of sequences to uppercase to ensure consistency.  
- Filtering out sequences shorter than 10 amino acids to remove peptides unlikely to exhibit measurable activity.  
- Redundant sequences were removed, ensuring a unique set of peptide sequences per organism.

Feature Extraction:  
ProtBERT, a transformer-based protein language model, was used to generate embeddings for all sequences. Steps included:  
- Tokenization of sequences using the ProtBERT tokenizer.  
- Padding/truncation of sequences to a fixed length of 512 tokens.  
- Extraction of embeddings from the last hidden state of the model.  
- Averaging over token embeddings to obtain a fixed-length vector representation for each sequence.

Feature Scaling and Dimensionality Reduction:  
- StandardScaler was applied to normalize features before model training.  
- Principal Component Analysis (PCA) was used to reduce dimensionality while retaining 95% of variance, primarily for models sensitive to high-dimensional inputs (e.g., SVR and MLP).

Targets:  
- MIC values were log-transformed using log1p to reduce skewness and stabilize variance for regression.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Design Decisions

Data Cleaning:  
- Only sequences with lengths >10 were retained to exclude potentially non-functional peptides.  
- Log transformation of MIC values ensures numerical stability and reduces the impact of extreme values.

Feature Selection / Dimensionality Reduction:  
- PCA was used to reduce high-dimensional ProtBERT embeddings for specific models.  
- No additional feature selection was applied beyond PCA since embeddings already capture rich sequence information.

Model Selection and Parameters:  
Four regression models were trained and evaluated using the same embeddings:  
- Support Vector Regressor (SVR): RBF kernel, PCA-reduced embeddings, handles non-linear relationships.  
- Random Forest Regressor: 100 trees, robust to high-dimensional data and outliers.  
- XGBoost Regressor: 100 estimators, objective='reg:squarederror', captures complex non-linear patterns.  
- MLP Regressor: Two hidden layers (512, 128), max_iter=500, PCA-reduced embeddings used to reduce overfitting and accelerate convergence.

Imbalanced Data Handling:  
- Not directly applicable for regression, but log transformation of MIC values reduces skew caused by extreme MICs.

Evaluation Metrics:  
- Regression performance was measured using:  
  - Mean Squared Error (MSE)  
  - Log-transformed MSE (MSE(log))  
  - R² Score  
  - Mean Absolute Error (MAE)  
  - Pearson and Kendall correlation coefficients  
- The best model per organism was determined using a combined rank of R² and MSE.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Algorithms Used

Embedding Extraction:  
- ProtBERT: Transformer-based protein language model that generates contextualized embeddings for sequences.

Regression Models:  
- SVR, Random Forest Regressor, XGBoost, MLP Regressor.

Dimensionality Reduction:  
- PCA for retaining 95% variance in high-dimensional embeddings.

Model Evaluation:  
- Split data into training and test sets (80:20).  
- Evaluated performance using multiple metrics including correlation and error measures.

Visualization / Explainability:  
- Scatter plots of true vs predicted MIC values.  
- Residual distribution plots.  
- Bland-Altman plots to evaluate agreement between predicted and true values.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Dependencies

Python Libraries:  
pandas, numpy, tqdm, transformers, torch, sklearn, xgboost, matplotlib, seaborn, scipy

Environment:  
Google Colab with GPU recommended for ProtBERT embedding generation due to computational intensity.

External Tools:  
ProtBERT model from HuggingFace (Rostlab/prot_bert)
